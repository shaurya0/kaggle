{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Conv2D, BatchNormalization\n",
    "from keras.layers import MaxPool2D, MaxPooling2D, Reshape, Dropout, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# matplotlib for displaying the output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline\n",
    "from WavDataLoader import WavDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['silence', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "data_dir = r'/home/shaur141/Development/kaggle/tensorflow-speech-recognition-challenge/data/train/audio'\n",
    "wav_data_loader = WavDataLoader(data_dir, labels, nx=128, ny=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        x = x_dict['x']\n",
    "        \n",
    "        x = tf.reshape(x, shape=[-1, 128, 32, 1])\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        \n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        \n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "        \n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "        \n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        \n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "        \n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = wav_data_loader.num_labels\n",
    "dropout = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False,\n",
    "                            is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True,\n",
    "                           is_training=False)\n",
    "    \n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probs = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "        \n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                  global_step=tf.train.get_global_step())\n",
    "\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_classes,\n",
    "        loss=loss_op,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# num_steps = 10\n",
    "# learning_rate = 0.001\n",
    "# input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#     x={'x': wav_data_loader.X}, y=wav_data_loader.y,\n",
    "#     batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "\n",
    "# model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inputs = Input(shape=(wav_data_loader.nx, wav_data_loader.ny,1))\n",
    "#     x = Reshape((wav_data_loader.nx*wav_data_loader.ny,))(inputs)\n",
    "#     x = BatchNormalization()(inputs)\n",
    "    x = Conv2D(16,(3,3),strides=(1,1), activation='relu')(inputs)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(strides=(1,1))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(32,(3,3),strides=(2,2), activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(strides=(1,1))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = SeparableConv2D(64,(3,3),strides=(2,2), activation='relu')(x) \n",
    "#     x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(strides=(1,1))(x)       \n",
    "    x = Reshape((-1,))(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "    predictions = Dense(wav_data_loader.num_labels, activation='softmax')(x)    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='Nadam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21941 samples, validate on 3873 samples\n",
      "Epoch 1/20\n",
      "21941/21941 [==============================] - 17s - loss: 1.2544 - acc: 0.5675 - val_loss: 0.7302 - val_acc: 0.7596\n",
      "Epoch 2/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.5842 - acc: 0.8011 - val_loss: 0.5238 - val_acc: 0.8239\n",
      "Epoch 3/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.4209 - acc: 0.8567 - val_loss: 0.4828 - val_acc: 0.8399\n",
      "Epoch 4/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.3462 - acc: 0.8828 - val_loss: 0.3429 - val_acc: 0.8892\n",
      "Epoch 5/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.2838 - acc: 0.9030 - val_loss: 0.3334 - val_acc: 0.8928\n",
      "Epoch 6/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.2478 - acc: 0.9137 - val_loss: 0.3189 - val_acc: 0.9009\n",
      "Epoch 7/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.2163 - acc: 0.9253 - val_loss: 0.3699 - val_acc: 0.8874\n",
      "Epoch 8/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.1930 - acc: 0.9326 - val_loss: 0.2951 - val_acc: 0.9096\n",
      "Epoch 9/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.1747 - acc: 0.9408 - val_loss: 0.2892 - val_acc: 0.9122\n",
      "Epoch 10/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.1622 - acc: 0.9446 - val_loss: 0.3311 - val_acc: 0.9029\n",
      "Epoch 11/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.1473 - acc: 0.9510 - val_loss: 0.3041 - val_acc: 0.9148\n",
      "Epoch 12/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.1434 - acc: 0.9506 - val_loss: 0.3305 - val_acc: 0.9109\n",
      "Epoch 13/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.1326 - acc: 0.9566 - val_loss: 0.3226 - val_acc: 0.9127\n",
      "Epoch 14/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.1169 - acc: 0.9606 - val_loss: 0.3517 - val_acc: 0.9065\n",
      "Epoch 15/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.1104 - acc: 0.9629 - val_loss: 0.3451 - val_acc: 0.9117\n",
      "Epoch 16/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.1058 - acc: 0.9650 - val_loss: 0.3857 - val_acc: 0.9076\n",
      "Epoch 17/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.1152 - acc: 0.9622 - val_loss: 0.3587 - val_acc: 0.9163\n",
      "Epoch 18/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.1029 - acc: 0.9669 - val_loss: 0.3918 - val_acc: 0.9109\n",
      "Epoch 19/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.0933 - acc: 0.9682 - val_loss: 0.3750 - val_acc: 0.9143\n",
      "Epoch 20/20\n",
      "21941/21941 [==============================] - 17s - loss: 0.0881 - acc: 0.9719 - val_loss: 0.4244 - val_acc: 0.9047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f910030aba8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=wav_data_loader.X, y=to_categorical(wav_data_loader.y), validation_split=0.15, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
