{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Conv2D, BatchNormalization\n",
    "from keras.layers import MaxPool2D, MaxPooling2D, Reshape, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# matplotlib for displaying the output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline\n",
    "from WavDataLoader import WavDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = ['silence', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "data_dir = r'C:\\Development\\kaggle\\tensorflow-speech-recognition-challenge\\data\\train\\audio'\n",
    "wav_data_loader = WavDataLoader(data_dir, labels, nx=128, ny=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        x = x_dict['x']\n",
    "        \n",
    "        x = tf.reshape(x, shape=[-1, 128, 32, 1])\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        \n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        \n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "        \n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "        \n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        \n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "        \n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = wav_data_loader.num_labels\n",
    "dropout = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False,\n",
    "                            is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True,\n",
    "                           is_training=False)\n",
    "    \n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probs = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "        \n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                  global_step=tf.train.get_global_step())\n",
    "\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_classes,\n",
    "        loss=loss_op,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# num_steps = 10\n",
    "# learning_rate = 0.001\n",
    "# input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#     x={'x': wav_data_loader.X}, y=wav_data_loader.y,\n",
    "#     batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "\n",
    "# model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inputs = Input(shape=(wav_data_loader.nx, wav_data_loader.ny,1))\n",
    "#     x = Reshape((wav_data_loader.nx*wav_data_loader.ny,))(inputs)\n",
    "#     x = BatchNormalization()(inputs)\n",
    "    x = Conv2D(8,(3,3),strides=(1,1), activation='relu')(inputs)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(strides=(1,1))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(16,(3,3),strides=(2,2), activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(strides=(1,1))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(32,(3,3),strides=(2,2), activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(strides=(1,1))(x)       \n",
    "    x = Reshape((-1,))(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(wav_data_loader.num_labels, activation='softmax')(x)    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='Nadam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23232 samples, validate on 2582 samples\n",
      "Epoch 1/1\n",
      "23232/23232 [==============================] - 91s 4ms/step - loss: 1.0769 - acc: 0.6368 - val_loss: 0.5905 - val_acc: 0.8118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27c1a20ed68>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=wav_data_loader.X, y=to_categorical(wav_data_loader.y), validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
