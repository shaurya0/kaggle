{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import data_utils\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport data_utils\n",
    "\n",
    "\n",
    "submission_data_path = os.path.join('data', 'test')\n",
    "train_data_path = os.path.join('data', 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data_from_csv():\n",
    "    train_files = data_utils.get_train_files(train_data_path)\n",
    "\n",
    "    train_data_as_array, train_data_as_list = data_utils.get_raw_train_data(train_files)\n",
    "    \n",
    "    return train_data_as_array, train_data_as_list\n",
    "\n",
    "train_data_as_array, train_data_as_list = load_train_data_from_csv()\n",
    "data_utils.pickle_obj(train_data_as_list, 'train_data_as_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_labels_from_csv():\n",
    "    label_files = data_utils.get_label_files(train_data_path)\n",
    "    train_labels = [data_utils.load_labels(f) for f in label_files]\n",
    "    return train_labels\n",
    "\n",
    "train_labels = load_train_labels_from_csv()\n",
    "data_utils.pickle_obj(train_labels, 'train_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(train_data_as_array)\n",
    "data_utils.pickle_obj(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def format_data(scaler, train_data, train_labels, sequence_size):    \n",
    "    num_train_inputs_total = 0\n",
    "    num_cols = train_data[0].shape[1]\n",
    "    num_outputs = train_labels[0].shape[1]\n",
    "    for td in train_data:\n",
    "        num_rows = td.shape[0]    \n",
    "        end_idx = num_rows - (num_rows%sequence_size)\n",
    "        num_train_inputs_total += end_idx//sequence_size\n",
    "\n",
    "\n",
    "    train_data_inputs = np.empty((num_train_inputs_total, sequence_size, num_cols))\n",
    "    train_label_inputs = np.empty((num_train_inputs_total, num_outputs))\n",
    "    start_idx = 0\n",
    "    for (td, labels) in zip(train_data, train_labels):\n",
    "        num_rows = td.shape[0]    \n",
    "        end_idx = num_rows - (num_rows%sequence_size)\n",
    "        num_train_inputs = end_idx//sequence_size\n",
    "\n",
    "        train_data_range = range(start_idx, start_idx + num_train_inputs)\n",
    "        td = scaler.transform(td)\n",
    "        train_data_inputs[train_data_range] = \\\n",
    "                np.array([td[i:i+sequence_size] for i in range(0, end_idx, sequence_size)])\n",
    "        train_label_inputs[train_data_range] = np.array(labels[:num_train_inputs])\n",
    "\n",
    "        start_idx += num_train_inputs    \n",
    "    return train_data_inputs, train_label_inputs\n",
    "\n",
    "def load_training_data(sequence_size):\n",
    "    train_data = data_utils.unpickle_obj('train_data_as_list.pkl')\n",
    "    train_labels = data_utils.unpickle_obj('train_labels.pkl')\n",
    "    scaler = data_utils.unpickle_obj('scaler.pkl')\n",
    "    X,Y = format_data(scaler, train_data, train_labels, sequence_size)\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "def split_train_data(train_data, train_labels, split_fraction=0.1):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train_data, train_labels, test_size=split_fraction, random_state=42)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def get_train_test_data(sequence_size):\n",
    "    train_data, train_labels = load_training_data(sequence_size)\n",
    "    X_train, X_test, Y_train, Y_test = split_train_data(train_data, train_labels)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "sequence_size = 100\n",
    "X_train, X_test, Y_train, Y_test = get_train_test_data(sequence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "def build_rnn_model(input_shape, layers):\n",
    "    model = Sequential()  \n",
    "    model.add(LSTM(layers[0], return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(layers[1], return_sequences=False))\n",
    "    model.add(Dense(layers[-1]))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaurya/anaconda3/lib/python3.5/site-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 153734 samples, validate on 8092 samples\n",
      "Epoch 1/1\n",
      "153734/153734 [==============================] - 210s - loss: 0.0323 - val_loss: 0.0332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f499c1a2518>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_rnn_model((100,30), (64, 32, 5))\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=1, validation_split=0.05)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
